{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " 'Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJleHAiOjE1NTU2Mjg3MjksImlhdCI6MTU1NTYyMTUyOSwibmJmIjoxNTU1NjIxNTI5LCJqdGkiOiIyNzA3ZWI0Mi1kZGNlLTQ1YzAtYTJkMi04ODY5NjE4YWMwNjEiLCJpZGVudGl0eSI6ImFkNTM0YzIyLTRmMzgtMTFlOS05ODE5LTBiYTcxODYyYmJiZSIsImZyZXNoIjpmYWxzZSwidHlwZSI6ImFjY2VzcyJ9.4FTV5id0KgFoKvqyFl3rGITRfp6hBv9FOYcWMQwGFOY')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from amb_sdk.sdk import DarwinSdk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#displays all datasets' columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "s = DarwinSdk()\n",
    "s.set_url('https://amb-demo-api.sparkcognition.com/v1/')\n",
    "s.auth_login_user('FPREIMESBERGER@GMAIL.COM','dRgC88hVfC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the storm datasets\n",
    "s2008 = pd.read_csv('StormEvents_details-ftp_v1.0_d2008_c20180718.csv.gz', compression='gzip')\n",
    "s2009 = pd.read_csv('StormEvents_details-ftp_v1.0_d2009_c20180718.csv.gz', compression='gzip')\n",
    "s2010 = pd.read_csv('StormEvents_details-ftp_v1.0_d2010_c20170726.csv.gz', compression='gzip')\n",
    "s2011 = pd.read_csv('StormEvents_details-ftp_v1.0_d2011_c20180718.csv.gz', compression='gzip')\n",
    "s2012 = pd.read_csv('StormEvents_details-ftp_v1.0_d2012_c20170519.csv.gz', compression='gzip')\n",
    "s2013 = pd.read_csv('StormEvents_details-ftp_v1.0_d2013_c20170519.csv.gz', compression='gzip')\n",
    "s2014 = pd.read_csv('StormEvents_details-ftp_v1.0_d2014_c20180718.csv.gz', compression='gzip')\n",
    "s2015 = pd.read_csv('StormEvents_details-ftp_v1.0_d2015_c20180525.csv.gz', compression='gzip')\n",
    "s2016 = pd.read_csv('StormEvents_details-ftp_v1.0_d2016_c20180718.csv.gz', compression='gzip')\n",
    "s2017 = pd.read_csv('StormEvents_details-ftp_v1.0_d2017_c20181219.csv.gz', compression='gzip')\n",
    "s2018 = pd.read_csv('StormEvents_details-ftp_v1.0_d2018_c20190220.csv.gz', compression='gzip')\n",
    "\n",
    "\n",
    "data = pd.concat([s2008,s2009,s2010,s2011,s2012,s2013,s2014,s2015,s2016,s2017,s2018])\n",
    "data.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- CLEANING ---------------------\n",
    "# split and BEGIN_YEARMONTH + END_YEARMONTH columns into 2 columns each --> YEAR + MONTH\n",
    "data['BEGIN_YEAR'] = data['BEGIN_YEARMONTH'].astype(str).str[:4]\n",
    "data['BEGIN_MONTH'] = data['BEGIN_YEARMONTH'].astype(str).str[4:]\n",
    "data['END_YEAR'] = data['END_YEARMONTH'].astype(str).str[:4]\n",
    "data['END_MONTH'] = data['END_YEARMONTH'].astype(str).str[4:]\n",
    "data.drop(columns=['END_YEARMONTH', 'BEGIN_YEARMONTH'], inplace=True)\n",
    "\n",
    "# add begin and end dates and times in YYYY-MM-DD HH:MM:SS format\n",
    "begin = data['BEGIN_YEAR'].map(str) + '-' + data['BEGIN_MONTH'].map(str) + '-' + data['BEGIN_DAY'].map(str) + ' '\n",
    "hour = data['BEGIN_TIME'].astype(str).str[:-2]\n",
    "hour = hour.apply(lambda x: '{0:0>2}'.format(x))\n",
    "minute = data['BEGIN_TIME'].astype(str).str[-2:]\n",
    "minute = minute.apply(lambda x: '{0:0>2}'.format(x))\n",
    "time = hour.map(str) + ':' + minute.map(str) + ':00'\n",
    "data['BEGIN'] = begin+time\n",
    "\n",
    "end = data['END_YEAR'].map(str) + '-' + data['END_MONTH'].map(str) + '-' + data['END_DAY'].map(str) + ' '\n",
    "e_hour = data['END_TIME'].astype(str).str[:-2]\n",
    "e_hour = e_hour.apply(lambda x: '{0:0>2}'.format(x))\n",
    "e_minute = data['END_TIME'].astype(str).str[-2:]\n",
    "e_minute = e_minute.apply(lambda x: '{0:0>2}'.format(x))\n",
    "e_time = e_hour.map(str) + ':' + e_minute.map(str) + ':00'\n",
    "data['END'] = end+e_time\n",
    "\n",
    "# add duration column\n",
    "duration = pd.to_datetime(data['END']) - pd.to_datetime(data['BEGIN'])\n",
    "                   \n",
    "data['DURATION_seconds'] = ((duration.dt.total_seconds()))    \n",
    "\n",
    "#drops\n",
    "data = data.drop(\"EPISODE_ID\", axis = 1)\n",
    "data = data.drop(\"EVENT_ID\", axis = 1)\n",
    "data = data.drop(\"STATE_FIPS\", axis = 1)\n",
    "data = data.drop(\"BEGIN_DAY\", axis = 1)\n",
    "data = data.drop(\"BEGIN_TIME\", axis = 1)\n",
    "data = data.drop(\"END_DAY\", axis = 1)\n",
    "data = data.drop(\"END_TIME\", axis = 1)\n",
    "\n",
    "data = data.drop(\"CATEGORY\", axis = 1)\n",
    "data = data.drop(\"CZ_TYPE\", axis = 1)\n",
    "data = data.drop(\"CZ_FIPS\", axis = 1)\n",
    "data = data.drop(\"CZ_NAME\", axis = 1)\n",
    "data = data.drop(\"TOR_OTHER_WFO\", axis = 1)\n",
    "data = data.drop(\"TOR_OTHER_CZ_NAME\", axis = 1)\n",
    "data = data.drop(\"TOR_OTHER_CZ_STATE\", axis = 1)\n",
    "data = data.drop(\"TOR_OTHER_CZ_FIPS\", axis = 1)\n",
    "data = data.drop(\"DATA_SOURCE\", axis = 1)\n",
    "\n",
    "#remove the row if no location information is provided\n",
    "#note this removes 250501 rows, reducing our dataset by 37%\n",
    "data = data[pd.notnull(data['BEGIN_LOCATION'])]\n",
    "data = data[pd.notnull(data['END_LOCATION'])]\n",
    "\n",
    "# --------------------- FEATURE ENGINEERING ---------------------\n",
    "#separates hail size / wind speed MAGNITUDE\n",
    "data['WIND_SPEED'] = np.where(data['MAGNITUDE_TYPE'].isin(['MG','EG']), data['MAGNITUDE'], np.NaN)\n",
    "data['HAIL_SIZE'] = np.where(data['MAGNITUDE_TYPE'].isna(), data['MAGNITUDE'], np.NaN)\n",
    "\n",
    "#divide the data into 12 distinct groups by event type:\n",
    "pd.value_counts(data.EVENT_TYPE)\n",
    "\"\"\"\n",
    "Wind (COLE):\n",
    "    Thunderstorm Wind\n",
    "    High Wind\n",
    "    Marine Thunderstorm Wind\n",
    "    Marine High Wind\n",
    "    Strong Wind\n",
    "    Marine Strong Wind\n",
    "\n",
    "Winter Weather (ETHAN):\n",
    "    Winter Weather\n",
    "    Winter Storm\n",
    "    Heavy Snow\n",
    "    Blizzard\n",
    "    Frost/Freeze\n",
    "    Ice Storm\n",
    "    Sleet\n",
    "    Lake-Effect Snow\n",
    "\n",
    "Rain (FREYA):\n",
    "    Heavy Rain\n",
    "\n",
    "Hail (RUOCHEN):\n",
    "    Hail\n",
    "    Marine Hail\n",
    "\n",
    "Flood (FREYA):\n",
    "    Flash Flood\n",
    "    Flood\n",
    "    Coastal Flood\n",
    "    Lakeshore Flood\n",
    "\n",
    "Drought (ETHAN):\n",
    "    Drought\n",
    "\n",
    "Tornado (COLE):\n",
    "    Tornado\n",
    "\n",
    "Heat (ETHAN):\n",
    "    Heat\n",
    "    Excessive Heat\n",
    "\n",
    "Cold (COLE):\n",
    "    Cold/Wind Chill\n",
    "    Extreme Cold/Wind Chill\n",
    "\n",
    "Lightning (FREYA):\n",
    "    Lightning\n",
    "\n",
    "Wildfire (RUOCHEN):\n",
    "    Wildfire\n",
    "\n",
    "Tides/Currents (RUOCHEN):\n",
    "    High Surf\n",
    "    Rip Current\n",
    "    Astronomical Low Tide\n",
    "    Storm Surge/Tide\n",
    "\"\"\"\n",
    "\n",
    "data.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
