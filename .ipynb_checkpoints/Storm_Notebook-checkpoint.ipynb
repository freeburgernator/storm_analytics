{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "#displays all datasets' columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "from amb_sdk.sdk import DarwinSdk\n",
    "s = DarwinSdk()\n",
    "s.auth_login_user('','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the storm datasets\n",
    "s2008 = pd.read_csv('StormEvents_details-ftp_v1.0_d2008_c20180718.csv.gz', compression='gzip')\n",
    "s2009 = pd.read_csv('StormEvents_details-ftp_v1.0_d2009_c20180718.csv.gz', compression='gzip')\n",
    "s2010 = pd.read_csv('StormEvents_details-ftp_v1.0_d2010_c20170726.csv.gz', compression='gzip')\n",
    "s2011 = pd.read_csv('StormEvents_details-ftp_v1.0_d2011_c20180718.csv.gz', compression='gzip')\n",
    "s2012 = pd.read_csv('StormEvents_details-ftp_v1.0_d2012_c20170519.csv.gz', compression='gzip')\n",
    "s2013 = pd.read_csv('StormEvents_details-ftp_v1.0_d2013_c20170519.csv.gz', compression='gzip')\n",
    "s2014 = pd.read_csv('StormEvents_details-ftp_v1.0_d2014_c20180718.csv.gz', compression='gzip')\n",
    "s2015 = pd.read_csv('StormEvents_details-ftp_v1.0_d2015_c20180525.csv.gz', compression='gzip')\n",
    "s2016 = pd.read_csv('StormEvents_details-ftp_v1.0_d2016_c20180718.csv.gz', compression='gzip')\n",
    "s2017 = pd.read_csv('StormEvents_details-ftp_v1.0_d2017_c20181219.csv.gz', compression='gzip')\n",
    "s2018 = pd.read_csv('StormEvents_details-ftp_v1.0_d2018_c20190220.csv.gz', compression='gzip')\n",
    "\n",
    "\n",
    "data = pd.concat([s2008,s2009,s2010,s2011,s2012,s2013,s2014,s2015,s2016,s2017,s2018])\n",
    "data.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- CLEANING ---------------------\n",
    "# split and BEGIN_YEARMONTH + END_YEARMONTH columns into 2 columns each --> YEAR + MONTH\n",
    "begin_year = data['BEGIN_YEARMONTH'].astype(str).str[:4]\n",
    "begin_month = data['BEGIN_YEARMONTH'].astype(str).str[4:]\n",
    "data['BEGIN_YEAR'] = begin_year\n",
    "data['BEGIN_MONTH'] = begin_month\n",
    "data.drop(columns=['BEGIN_YEARMONTH'], inplace=True)\n",
    "\n",
    "end_year = data['END_YEARMONTH'].astype(str).str[:4]\n",
    "end_month = data['END_YEARMONTH'].astype(str).str[4:]\n",
    "data['END_YEAR'] = end_year\n",
    "data['END_MONTH'] = end_month\n",
    "data.drop(columns=['END_YEARMONTH'], inplace=True)\n",
    "\n",
    "# add begin and end dates and times in YYYY-MM-DD HH:MM:SS format\n",
    "begin = data['BEGIN_YEAR'].map(str) + '-' + data['BEGIN_MONTH'].map(str) + '-' + data['BEGIN_DAY'].map(str) + ' '\n",
    "hour = data['BEGIN_TIME'].astype(str).str[:-2]\n",
    "hour = hour.apply(lambda x: '{0:0>2}'.format(x))\n",
    "minute = data['BEGIN_TIME'].astype(str).str[-2:]\n",
    "minute = minute.apply(lambda x: '{0:0>2}'.format(x))\n",
    "time = hour.map(str) + ':' + minute.map(str) + ':00'\n",
    "data['BEGIN'] = begin+time\n",
    "\n",
    "end = data['END_YEAR'].map(str) + '-' + data['END_MONTH'].map(str) + '-' + data['END_DAY'].map(str) + ' '\n",
    "e_hour = data['END_TIME'].astype(str).str[:-2]\n",
    "e_hour = e_hour.apply(lambda x: '{0:0>2}'.format(x))\n",
    "e_minute = data['END_TIME'].astype(str).str[-2:]\n",
    "e_minute = e_minute.apply(lambda x: '{0:0>2}'.format(x))\n",
    "e_time = e_hour.map(str) + ':' + e_minute.map(str) + ':00'\n",
    "data['END'] = end+e_time\n",
    "\n",
    "# add duration column\n",
    "duration = pd.to_datetime(data['END']) - pd.to_datetime(data['BEGIN'])\n",
    "                   \n",
    "data['DURATION_seconds'] = ((duration.dt.total_seconds()))    \n",
    "\n",
    "#drops\n",
    "data = dataset.drop(\"BEGIN_YEARMONTH\", axis = 1)\n",
    "data = data.drop(\"END_YEARMONTH\", axis = 1)\n",
    "data = data.drop(\"EPISODE_ID\", axis = 1)\n",
    "data = data.drop(\"EVENT_ID\", axis = 1)\n",
    "data = data.drop(\"STATE_FIPS\", axis = 1)\n",
    "data = data.drop(\"BEGIN_DAY\", axis = 1)\n",
    "data = data.drop(\"BEGIN_TIME\", axis = 1)\n",
    "data = data.drop(\"END_DAY\", axis = 1)\n",
    "data = data.drop(\"END_TIME\", axis = 1)\n",
    "\n",
    "data = data.drop(\"CATEGORY\", axis = 1)\n",
    "data = data.drop(\"CZ_TYPE\", axis = 1)\n",
    "data = data.drop(\"CZ_FIPS\", axis = 1)\n",
    "data = data.drop(\"CZ_NAME\", axis = 1)\n",
    "data = data.drop(\"TOR_OTHER_WFO\", axis = 1)\n",
    "data = data.drop(\"TOR_OTHER_CZ_NAME\", axis = 1)\n",
    "data = data.drop(\"TOR_OTHER_CZ_STATE\", axis = 1)\n",
    "data = data.drop(\"TOR_OTHER_CZ_FIPS\", axis = 1)\n",
    "data = data.drop(\"DATA_SOURCE\", axis = 1)\n",
    "\n",
    "\n",
    "# --------------------- FEATURE ENGINEERING ---------------------\n",
    "\n",
    "#separates hail size / wind speed MAGNITUDE\n",
    "data['WIND_SPEED'] = np.where(data['MAGNITUDE_TYPE'].isin(['MG','EG']), data['MAGNITUDE'], np.NaN)\n",
    "data['HAIL_SIZE'] = np.where(data['MAGNITUDE_TYPE'].isna(), data['MAGNITUDE'], np.NaN)\n",
    "\n",
    "\n",
    "data.head(50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
